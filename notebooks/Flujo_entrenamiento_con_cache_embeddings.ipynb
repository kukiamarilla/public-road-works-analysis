{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97ba3346",
   "metadata": {},
   "source": [
    "# Flujo de entrenamiento (con precálculo / cache de embeddings por chunk)\n",
    "\n",
    "Este notebook implementa el flujo **2-etapas**:\n",
    "\n",
    "1) **Precalcular embeddings**: `texto licitación -> chunking -> ModeloB (GPT-OSS congelado) -> embeddings por chunk` y guardar en disco (`cache_dir/xxx.pt`)\n",
    "\n",
    "2) **Entrenar ModeloC** (cross-chunk + MLP) leyendo solo embeddings cacheados (rápido)\n",
    "\n",
    "3) **Inferencia**: `texto -> ModeloB -> embeddings -> ModeloC -> y_hat`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06eca908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.57.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.12.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.3.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0.0->accelerate) (12.9.86)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requisitos\n",
    "!pip install -U transformers accelerate pandas\n",
    "\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef86b3e8",
   "metadata": {},
   "source": [
    "## Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c90390e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CFG(model_id='openai/gpt-oss-20b', device='cuda', dtype=torch.bfloat16, max_len=4096, stride=2048, d_model=512, n_heads=8, ffn_dim=2048, dropout=0.1, batch_size=8, lr=0.0002, epochs=10, cache_dir='./cache_chunk_embs')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@dataclass\n",
    "class CFG:\n",
    "    model_id: str = \"openai/gpt-oss-20b\"\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dtype: torch.dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    # Chunking\n",
    "    max_len: int = 4096\n",
    "    stride: int = 2048\n",
    "\n",
    "    # Cross-chunk (liviano)\n",
    "    d_model: int = 512\n",
    "    n_heads: int = 8\n",
    "    ffn_dim: int = 2048\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Training\n",
    "    batch_size: int = 8\n",
    "    lr: float = 2e-4\n",
    "    epochs: int = 10\n",
    "\n",
    "    # Cache\n",
    "    cache_dir: str = \"./cache_chunk_embs\"\n",
    "\n",
    "cfg = CFG()\n",
    "cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a35ce7b",
   "metadata": {},
   "source": [
    "## ModeloB – GPT-OSS congelado (embeddings por chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a42dcaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelB_ChunkEmbedder(nn.Module):\n",
    "    def __init__(self, gpt_model, tokenizer, max_len=4096, stride=2048, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.gpt = gpt_model.eval()\n",
    "        for p in self.gpt.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.stride = stride\n",
    "        self.device = device\n",
    "\n",
    "        if self.tokenizer.pad_token_id is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.pad_id = self.tokenizer.pad_token_id\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, text: str) -> torch.Tensor:\n",
    "        enc = self.tokenizer(text, return_tensors=\"pt\", truncation=False)\n",
    "        input_ids = enc[\"input_ids\"][0].to(self.device)\n",
    "        attn_mask = enc[\"attention_mask\"][0].to(self.device)\n",
    "        L = int(attn_mask.sum().item())\n",
    "\n",
    "        chunk_embs = []\n",
    "        \n",
    "        for start in range(0, max(1, L), self.stride):\n",
    "            end = min(start + self.max_len, L)\n",
    "            ids = input_ids[start:end]\n",
    "            am  = attn_mask[start:end]\n",
    "            if ids.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            pad_len = self.max_len - ids.numel()\n",
    "            if pad_len > 0:\n",
    "                ids = torch.cat([ids, torch.full((pad_len,), self.pad_id, device=self.device, dtype=ids.dtype)])\n",
    "                am  = torch.cat([am, torch.zeros((pad_len,), device=self.device, dtype=am.dtype)])\n",
    "\n",
    "            # Procesar UN chunk a la vez (no batching)\n",
    "            out = self.gpt(input_ids=ids.unsqueeze(0), attention_mask=am.unsqueeze(0), return_dict=True)\n",
    "            h = out.last_hidden_state  # [1,T,d]\n",
    "            \n",
    "            last_idx = int(am.sum().item()) - 1\n",
    "            chunk_embs.append(h[0, last_idx].clone())  # [d]\n",
    "            \n",
    "            # Liberar memoria\n",
    "            del out, h\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if end == L:\n",
    "                break\n",
    "\n",
    "        return torch.stack(chunk_embs)  # [N,d]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90de6414",
   "metadata": {},
   "source": [
    "## ModeloC – Cross-chunk (no causal) + MLP regressor (liviano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7567115",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelC_CrossChunkRegressor(nn.Module):\n",
    "    def __init__(self, d_in: int, d_model: int = 512, n_heads: int = 8, ffn_dim: int = 2048, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_in, d_model)\n",
    "\n",
    "        layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=ffn_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "            activation=\"gelu\",\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(layer, num_layers=1)\n",
    "\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        nn.init.normal_(self.cls, std=0.02)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, chunk_embs: torch.Tensor, valid_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        B, N, _ = chunk_embs.shape\n",
    "        x = self.proj(chunk_embs)\n",
    "\n",
    "        cls = self.cls.expand(B, 1, -1)\n",
    "        x = torch.cat([cls, x], dim=1)\n",
    "\n",
    "        if valid_mask is not None:\n",
    "            cls_valid = torch.ones((B, 1), device=x.device, dtype=torch.bool)\n",
    "            valid = torch.cat([cls_valid, valid_mask], dim=1)\n",
    "            pad_mask = ~valid\n",
    "        else:\n",
    "            pad_mask = None\n",
    "\n",
    "        x = self.encoder(x, src_key_padding_mask=pad_mask)\n",
    "        pooled = x[:, 0]\n",
    "        return self.head(pooled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe936579",
   "metadata": {},
   "source": [
    "## Cargar datos (placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e9e199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, torch.Size([244]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reemplazá esto por tu loader real (CSV/DB/paths/etc.)\n",
    "from pydoc import text\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"public-road-works-analysis/data/dataset.csv\")\n",
    "\n",
    "ids = list(df[\"Id llamado\"])\n",
    "texts = []\n",
    "for id in ids[:50]:\n",
    "    f = open(f\"public-road-works-analysis/data/pbcs_extracted/{id}.txt\")\n",
    "    texts.append(f.read())\n",
    "targets = torch.tensor(list(df[\"Cantidad de oferentes\"]))\n",
    "len(texts), targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0f2d192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "243"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d70179",
   "metadata": {},
   "source": [
    "## Inicializar GPT-OSS congelado + ModeloC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8347e1b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41503b9efed9408990c2fb25b9600b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22aa729942394767ac44e0415a74d1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/27.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0200875791644a28bc81b542fe80d8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f9c6b2bdba49fc91893fa544abb5a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4446ec71086f48918513828649b8f241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MXFP4 quantization requires Triton and kernels installed: CUDA requires Triton >= 3.4.0, XPU requires Triton >= 3.5.0, we will default to dequantizing the model to bf16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89e8bbc81114ce6bc84408bfc63288d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a07187341984935b17616afbaae4e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9620507eec6b483b857eb3df6b6cf153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f9f28741eb400dbe15b0ce72024124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00000-of-00002.safetensors:   0%|          | 0.00/4.79G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f774d4d54574ba190726e0a363add23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.80G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76352427d48a4318bba7096256395738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2880"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_id)\n",
    "base = AutoModel.from_pretrained(cfg.model_id, dtype=cfg.dtype).to(cfg.device)\n",
    "\n",
    "modelB = ModelB_ChunkEmbedder(base, tokenizer, max_len=cfg.max_len, stride=cfg.stride, device=cfg.device)\n",
    "\n",
    "d_in = base.config.hidden_size\n",
    "modelC = ModelC_CrossChunkRegressor(d_in=d_in, d_model=cfg.d_model, n_heads=cfg.n_heads, ffn_dim=cfg.ffn_dim, dropout=cfg.dropout).to(cfg.device)\n",
    "\n",
    "d_in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf86a78",
   "metadata": {},
   "source": [
    "## (1) Precálculo / cache de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169ede07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5db5948351854f298eef6b0b3ee64f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Cacheando embeddings:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache listo: ./cache_chunk_embs\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def precache_embeddings(texts: List[str], targets: torch.Tensor, tender_ids: List, modelB: ModelB_ChunkEmbedder, cache_dir: str):\n",
    "    \"\"\"\n",
    "    Genera embeddings y los guarda con el ID de licitación como nombre de archivo.\n",
    "    Args:\n",
    "        texts: Lista de textos de licitaciones\n",
    "        targets: Tensor con cantidad de oferentes\n",
    "        tender_ids: Lista de IDs de licitación (mismo orden que texts)\n",
    "        modelB: Modelo para generar embeddings\n",
    "        cache_dir: Directorio donde guardar los .pt\n",
    "    \"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    for tender_id, txt, y in tqdm(zip(tender_ids, texts, targets), total=len(texts), desc=\"Cacheando embeddings\"):\n",
    "        path = os.path.join(cache_dir, f\"{tender_id}.pt\")\n",
    "        if os.path.exists(path):\n",
    "            continue\n",
    "\n",
    "        embs = modelB(txt).cpu()  # [N,d]\n",
    "        torch.save({\"embs\": embs, \"y\": float(y.item()), \"tender_id\": tender_id}, path)\n",
    "\n",
    "    print(\"cache listo:\", cache_dir)\n",
    "\n",
    "# Pasar los IDs de licitación como tercer argumento\n",
    "precache_embeddings(texts, targets, ids[:50], modelB, cfg.cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d29a98",
   "metadata": {},
   "source": [
    "## (2) Dataset cacheado + collate (padding por N chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f443207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2976/1370045055.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(self.files[idx], map_location=\"cpu\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, torch.Size([4, 62, 2880]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CachedChunkEmbDataset(Dataset):\n",
    "    def __init__(self, cache_dir: str):\n",
    "        self.files = sorted([os.path.join(cache_dir, f) for f in os.listdir(cache_dir) if f.endswith(\".pt\")])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        d = torch.load(self.files[idx], map_location=\"cpu\")\n",
    "        return d[\"embs\"].float(), torch.tensor(d[\"y\"], dtype=torch.float32)\n",
    "\n",
    "def collate_pad_chunks(batch: List[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "    embs_list, y_list = zip(*batch)\n",
    "    B = len(embs_list)\n",
    "    d = embs_list[0].shape[1]\n",
    "    Nmax = max(e.shape[0] for e in embs_list)\n",
    "\n",
    "    embs = torch.zeros((B, Nmax, d), dtype=torch.float32)\n",
    "    valid = torch.zeros((B, Nmax), dtype=torch.bool)\n",
    "\n",
    "    for i, e in enumerate(embs_list):\n",
    "        n = e.shape[0]\n",
    "        embs[i, :n] = e\n",
    "        valid[i, :n] = True\n",
    "\n",
    "    y = torch.stack(y_list).view(B, 1)\n",
    "    return embs, valid, y\n",
    "\n",
    "ds = CachedChunkEmbDataset(cfg.cache_dir)\n",
    "dl = DataLoader(ds, batch_size=cfg.batch_size, shuffle=True, collate_fn=collate_pad_chunks)\n",
    "\n",
    "len(ds), next(iter(dl))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e6477",
   "metadata": {},
   "source": [
    "## (3) Entrenamiento de ModeloC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "799c0277",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2976/1370045055.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  d = torch.load(self.files[idx], map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 | loss=1.3138\n",
      "epoch 02 | loss=0.4863\n",
      "epoch 03 | loss=0.3313\n",
      "epoch 04 | loss=0.4157\n",
      "epoch 05 | loss=0.2966\n",
      "epoch 06 | loss=0.3600\n",
      "epoch 07 | loss=0.3172\n",
      "epoch 08 | loss=0.2919\n",
      "epoch 09 | loss=0.2631\n",
      "epoch 10 | loss=0.1600\n"
     ]
    }
   ],
   "source": [
    "def train_modelC(modelC: nn.Module, dl: DataLoader, cfg: CFG):\n",
    "    modelC.train()\n",
    "    opt = torch.optim.AdamW(modelC.parameters(), lr=cfg.lr, weight_decay=0.01)\n",
    "    loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "    for ep in range(1, cfg.epochs + 1):\n",
    "        total = 0.0\n",
    "        for embs, valid, y in dl:\n",
    "            embs = embs.to(cfg.device)\n",
    "            valid = valid.to(cfg.device)\n",
    "            y = y.to(cfg.device)\n",
    "\n",
    "            y_hat = modelC(embs, valid)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total += float(loss.item())\n",
    "\n",
    "        print(f\"epoch {ep:02d} | loss={total/len(dl):.4f}\")\n",
    "\n",
    "train_modelC(modelC, dl, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20355dd2",
   "metadata": {},
   "source": [
    "## Guardar pesos de ModeloC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83c04dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints/modelC_crosschunk.pt'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(\"./checkpoints\", exist_ok=True)\n",
    "ckpt_path = \"./checkpoints/modelC_crosschunk.pt\"\n",
    "torch.save(modelC.state_dict(), ckpt_path)\n",
    "ckpt_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae40f9",
   "metadata": {},
   "source": [
    "## (4) Inferencia final (ModeloA = B + C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a29875f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4982], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelA_Full(nn.Module):\n",
    "    def __init__(self, modelB: ModelB_ChunkEmbedder, modelC: nn.Module):\n",
    "        super().__init__()\n",
    "        self.B = modelB\n",
    "        self.C = modelC\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict_one(self, text: str) -> torch.Tensor:\n",
    "        chunk_vecs = self.B(text)              # [N,d] - bfloat16\n",
    "        chunk_vecs = chunk_vecs.float()        # Convertir a float32\n",
    "        chunk_vecs = chunk_vecs.unsqueeze(0)   # [1,N,d]\n",
    "        valid = torch.ones((1, chunk_vecs.size(1)), device=chunk_vecs.device, dtype=torch.bool)\n",
    "        self.C.eval()\n",
    "        y_hat = self.C(chunk_vecs, valid)      # [1,1]\n",
    "        return y_hat.squeeze(0)\n",
    "\n",
    "modelA = ModelA_Full(modelB, modelC)\n",
    "\n",
    "f = open(\"341016.txt\", \"r\")\n",
    "test_text = f.read()\n",
    "pred = modelA.predict_one(test_text)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2391e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
